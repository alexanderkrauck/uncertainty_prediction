\documentclass[11pt]{article}

\usepackage{sectsty}
\usepackage{graphicx}
\usepackage{amsmath}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{ VAE Conditional Density Estimation - Draft}
\author{ Alexander Krauck}
\date{\today}

\begin{document}
\maketitle	

We have a dataset $\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}$. Additionally, we utilize an encoder and decoder from a Variational Autoencoder (VAE). The encoder takes as input $(x_i, y_i)$ and outputs $(\mathbf{\mu}_i, \mathbf{\Sigma}_i)$, where $\mathbf{\mu}_i \in \mathbb{R}^l$ and $\mathbf{\Sigma}_i \in \mathbb{R}^{l \times l}$ with the input $x_i \in \mathbb{R}^k$ and the latent space dimension $l$. The decoder maps from the $l$-dimensional latent space back to the original space of dimension $k$. Training is conducted using the standard VAE i.e. the Evidence Lower Bound (ELBO) loss, which encourages the latent space to usually approximate a Gaussian distribution $\mathcal{N}(0, I)$.

Given a test sample $x_{\text{test}}$, we aim to infer $p(y|x_{\text{test}})$ for this sample using the VAE. Here, we do not have $y_{\text{test}}$ or we pretend not to. We attempt multiple values for $y$ to approximate the joint probability density function $p(y, x_{\text{test}})$ with $x_{\text{test}}$ fixed. To ensure stability of the encoder for values of $y$ outside the typical range, we limit the inputs to a "plausible range", a hyperparameter. For instance, in a market prediction task, this could range from -1 to 1 after normalization and stand for the percent price change of some stock. For classification tasks this would become simpler as we would just use the one hot encoding. We then select an interval (another hyperparameter) for sampling $y$ values, such as 0.01. We make predictions for all values $p(x_{\text{test}}, y_k)$ where $y_k$ comes from this grid.

To obtain $p(y|x_{\text{test}})$ from $p(y, x_{\text{test}})$, we need $p(x_{\text{test}})$, which can be approximated by numerically integrating out $y$ from $p(x_{\text{test}}, y_k)$. This can be done as follows:
\[ p(x_{\text{test}}) = \int_{-\infty}^{\infty} p(x_{\text{test}}, y) \, dy \approx \int_{-1}^{1} p(x_{\text{test}}, y) \, dy \approx \frac{1}{201} \sum_{i=0}^{200} p(x_{\text{test}}, y_i) \]
Finally, for each value, we can compute $p(y|x_{\text{test}})$ using the formula $p(y|x_{\text{test}}) = \frac{p(x_{\text{test}}, y)}{p(x_{\text{test}})}$. The goal is to obtain complex, possibly multimodal, probability distributions for $p(y|x_{\text{test}})$, enhancing the capabilities of conditional density estimation.

\section{Possible issues that come to my mind}
It might be possible that this "plausible range" needs some more thought and also during training but I am not really sure. 

\section{Possible additions}
If we see that $p(x_{\text{test}})$ is low then this might be a naive indicator that we are not very certain about this sample in the epistemic sense.


We might be able to combine this with a bayesian neural network approach by having a bayesian encoder. Then for each $(x,y)$ pair that we input into the encoder we might obtain a different variance in the mean itself of the encoder. High variance means that we are not very certain for this particular realization of $p(x,y)$. Moreover, I am not really sure how the variance output of the VAE encoder comes into play here.



%--/Paper--

\end{document}