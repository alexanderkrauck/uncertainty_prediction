@article{wolpert_1996,
  author   = {Wolpert, David H.},
  title    = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
  journal  = {Neural Computation},
  volume   = {8},
  number   = {7},
  pages    = {1341-1390},
  year     = {1996},
  month    = {10},
  abstract = {{This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1996.8.7.1341},
  url      = {https://doi.org/10.1162/neco.1996.8.7.1341},
  eprint   = {https://direct.mit.edu/neco/article-pdf/8/7/1341/813495/neco.1996.8.7.1341.pdf}
}


@article{hullermeier_aleatoric_2021,
  title      = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
  volume     = {110},
  issn       = {1573-0565},
  shorttitle = {Aleatoric and epistemic uncertainty in machine learning},
  url        = {https://doi.org/10.1007/s10994-021-05946-3},
  doi        = {10.1007/s10994-021-05946-3},
  abstract   = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  language   = {en},
  number     = {3},
  urldate    = {2023-10-18},
  journal    = {Machine Learning},
  author     = {Hüllermeier, Eyke and Waegeman, Willem},
  month      = mar,
  year       = {2021},
  keywords   = {Bayesian inference, Calibration, Conformal prediction, Credal sets and classifiers, Deep neural networks, Ensembles, Epistemic uncertainty, Gaussian processes, Generative models, Likelihood-based methods, Probability, Set-valued prediction, Uncertainty, Version space learning},
  pages      = {457--506}
}


@inproceedings{gal_dropout_2016,
  title     = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author    = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  pages     = {1050--1059},
  year      = {2016},
  editor    = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v48/gal16.pdf},
  url       = {https://proceedings.mlr.press/v48/gal16.html},
  abstract  = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@inproceedings{lakshminarayanan_ensembles_2017,
  author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@inproceedings{hernandez2015probabilistic,
  title        = {Probabilistic backpropagation for scalable learning of bayesian neural networks},
  author       = {Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
  booktitle    = {International conference on machine learning},
  pages        = {1861--1869},
  year         = {2015},
  organization = {PMLR}
}

@misc{alpaca2023,
  author       = {Kirill, Fedyanin and Evgenii, Tsymbalov and Mikhail, Tereshkin},
  title        = {{ALPACA}: Library for Active Learning and Uncertainty Estimation in Machine Learning},
  year         = {2023},
  howpublished = {\url{https://github.com/stat-ml/alpaca/tree/master}},
  note         = {Accessed: 2023-10-26}
}

@misc{kelly2023uci,
  author       = {Markelle Kelly and Rachel Longjohn and Kolby Nottingham},
  title        = {The UCI Machine Learning Repository},
  year         = {2023},
  howpublished = {\url{https://archive.ics.uci.edu}},
  note         = {Accessed: 2023-10-26}
}

@misc{schweighofer2023quantification,
  title         = {Quantification of Uncertainty with Adversarial Models},
  author        = {Kajetan Schweighofer and Lukas Aichberger and Mykyta Ielanskyi and Günter Klambauer and Sepp Hochreiter},
  year          = {2023},
  eprint        = {2307.03217},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{HORNIK1989359,
  title    = {Multilayer feedforward networks are universal approximators},
  journal  = {Neural Networks},
  volume   = {2},
  number   = {5},
  pages    = {359-366},
  year     = {1989},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/0893-6080(89)90020-8},
  url      = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  author   = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}


@inproceedings{pmlr-v80-depeweg18a,
  title     = {Decomposition of Uncertainty in {B}ayesian Deep Learning for Efficient and Risk-sensitive Learning},
  author    = {Depeweg, Stefan and Hernandez-Lobato, Jose-Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {1184--1193},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/depeweg18a/depeweg18a.pdf},
  url       = {https://proceedings.mlr.press/v80/depeweg18a.html},
  abstract  = {Bayesian neural networks with latent variables are scalable and flexible probabilistic models: they account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. Using these models we show how to perform and utilize a decomposition of uncertainty in aleatoric and epistemic components for decision making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learningto identify policies that balance expected cost, model-bias and noise aversion.}
}

@inproceedings{soton258961,
  booktitle = {Sixteenth International Joint Conference on Artificial Intelligence (IJCAI '99) (01/01/99)},
  title     = {Transduction with Confidence and Credibility},
  author    = {C. Saunders and A. Gammerman and V. Vovk},
  year      = {1999},
  pages     = {722--726},
  url       = {https://eprints.soton.ac.uk/258961/},
  abstract  = {In this paper we follow the same general ideology as in (Gammerman et. al, 1998), and describe a new transductive learning algorithm using Support Vector Machines. The algorithm presented provides confidence values for its predicted classifications of new examples. We also obtain a measure of "credibility" which serves as an indicator of the reliability of the data upon which we make our prediction. Experiments compare the new algorithm to a standard Support Vector Machine and other transductive methods which use Support Vector Machines, such as Vapnik's margin transduction. Empirical results show that the new algorithm not only produces confidence and credibility measures, but is comparable to, and sometimes exceeds the performance of the other algorithms.}
}

@incollection{Papadopoulos08,
  author    = {Harris Papadopoulos},
  title     = {Inductive Conformal Prediction: Theory and Application to Neural Networks},
  booktitle = {Tools in Artificial Intelligence},
  publisher = {IntechOpen},
  address   = {Rijeka},
  year      = {2008},
  editor    = {Paula Fritzsche},
  chapter   = {18},
  doi       = {10.5772/6078},
  url       = {https://doi.org/10.5772/6078}
}

@misc{rothfuss2019conditional,
  title         = {Conditional Density Estimation with Neural Networks: Best Practices and Benchmarks},
  author        = {Jonas Rothfuss and Fabio Ferreira and Simon Walther and Maxim Ulrich},
  year          = {2019},
  eprint        = {1903.00954},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@misc{ambrogioni2017kernel,
  title         = {The Kernel Mixture Network: A Nonparametric Method for Conditional Density Estimation of Continuous Random Variables},
  author        = {Luca Ambrogioni and Umut Güçlü and Marcel A. J. van Gerven and Eric Maris},
  year          = {2017},
  eprint        = {1705.07111},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@misc{trippe2018conditional,
  title         = {Conditional Density Estimation with Bayesian Normalising Flows},
  author        = {Brian L Trippe and Richard E Turner},
  year          = {2018},
  eprint        = {1802.04908},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{bishop1994mixture,
  title     = {Mixture density networks},
  author    = {Bishop, Christopher M},
  year      = {1994},
  publisher = {Aston University}
}

@article{ambrogioni2017kernel,
  title   = {The kernel mixture network: A nonparametric method for conditional density estimation of continuous random variables},
  author  = {Ambrogioni, Luca and G{\"u}{\c{c}}l{\"u}, Umut and van Gerven, Marcel AJ and Maris, Eric},
  journal = {arXiv preprint arXiv:1705.07111},
  year    = {2017}
}

@article{klotz2021uncertainty,
  author  = {Klotz, D. and Kratzert, F. and Gauch, M. and Keefe Sampson, A. and Brandstetter, J. and Klambauer, G. and Hochreiter, S. and Nearing, G.},
  title   = {Uncertainty estimation with deep learning for rainfall--runoff modeling},
  journal = {Hydrology and Earth System Sciences},
  volume  = {26},
  year    = {2022},
  number  = {6},
  pages   = {1673--1693},
  url     = {https://hess.copernicus.org/articles/26/1673/2022/},
  doi     = {10.5194/hess-26-1673-2022}
}

@book{mcneil2015quantitative,
  title     = {Quantitative risk management: concepts, techniques and tools-revised edition},
  author    = {McNeil, Alexander J and Frey, R{\"u}diger and Embrechts, Paul},
  year      = {2015},
  publisher = {Princeton university press}
}

@article{rothfuss2019noise,
  title   = {Noise regularization for conditional density estimation},
  author  = {Rothfuss, Jonas and Ferreira, Fabio and Boehm, Simon and Walther, Simon and Ulrich, Maxim and Asfour, Tamim and Krause, Andreas},
  journal = {arXiv preprint arXiv:1907.08982},
  year    = {2019}
}

@article{gneiting2007probabilistic,
  title     = {Probabilistic forecasts, calibration and sharpness},
  author    = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E},
  journal   = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume    = {69},
  number    = {2},
  pages     = {243--268},
  year      = {2007},
  publisher = {Oxford University Press}
}

@article{chung2020beyond,
  title   = {Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification},
  author  = {Chung, Youngseog and Neiswanger, Willie and Char, Ian and Schneider, Jeff},
  journal = {arXiv preprint arXiv:2011.09588},
  year    = {2020}
}

@article{tran2020methods,
  title     = {Methods for comparing uncertainty quantifications for material property predictions},
  author    = {Tran, Kevin and Neiswanger, Willie and Yoon, Junwoong and Zhang, Qingyang and Xing, Eric and Ulissi, Zachary W},
  journal   = {Machine Learning: Science and Technology},
  volume    = {1},
  number    = {2},
  pages     = {025006},
  year      = {2020},
  publisher = {IOP Publishing}
}

@article{chung2021uncertainty,
  title   = {Uncertainty Toolbox: an Open-Source Library for Assessing, Visualizing, and Improving Uncertainty Quantification},
  author  = {Chung, Youngseog and Char, Ian and Guo, Han and Schneider, Jeff and Neiswanger, Willie},
  journal = {arXiv preprint arXiv:2109.10254},
  year    = {2021}
}

@misc{dua2019uci,
  author      = {Dua, Dheeru and Graff, Casey},
  year        = {2017},
  title       = {UCI Machine Learning Repository},
  url         = {http://archive.ics.uci.edu/ml},
  institution = {University of California, Irvine, School of Information and Computer Sciences}
} 


@techreport{bishop1994mixture,
  title       = {Mixture density networks},
  abstract    = {Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classifications problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the effectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.},
  keywords    = {NCRG sum-of-squares cross-entropy error function classifications problems coding scheme conditional probability distribution network models neural network mixture density model Mixture Density Network inverse kinematics},
  author      = {Bishop, {Christopher M.}},
  year        = {1994},
  language    = {English},
  isbn        = {NCRG/94/004},
  publisher   = {Aston University},
  type        = {WorkingPaper},
  institution = {Aston University}
}

@inproceedings{rezende2015variational,
  title        = {Variational inference with normalizing flows},
  author       = {Rezende, Danilo and Mohamed, Shakir},
  booktitle    = {International conference on machine learning},
  pages        = {1530--1538},
  year         = {2015},
  organization = {PMLR}
}

@article{ambrogioni2017kernel,
  title   = {The kernel mixture network: A nonparametric method for conditional density estimation of continuous random variables},
  author  = {Ambrogioni, Luca and G{\"u}{\c{c}}l{\"u}, Umut and van Gerven, Marcel AJ and Maris, Eric},
  journal = {arXiv preprint arXiv:1705.07111},
  year    = {2017}
}

@article{silverman1982estimation,
  title     = {On the estimation of a probability density function by the maximum penalized likelihood method},
  author    = {Silverman, Bernard W},
  journal   = {The Annals of Statistics},
  pages     = {795--810},
  year      = {1982},
  publisher = {JSTOR}
}

@book{li2023nonparametric,
  title     = {Nonparametric econometrics: theory and practice},
  author    = {Li, Qi and Racine, Jeffrey Scott},
  year      = {2023},
  publisher = {Princeton University Press}
}

@inproceedings{sugiyama2010conditional,
  title        = {Conditional density estimation via least-squares density ratio estimation},
  author       = {Sugiyama, Masashi and Takeuchi, Ichiro and Suzuki, Taiji and Kanamori, Takafumi and Hachiya, Hirotaka and Okanohara, Daisuke},
  booktitle    = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages        = {781--788},
  year         = {2010},
  organization = {JMLR Workshop and Conference Proceedings}
}

@article{selten1998axiomatic,
  title     = {Axiomatic characterization of the quadratic scoring rule},
  author    = {Selten, Reinhard},
  journal   = {Experimental Economics},
  volume    = {1},
  pages     = {43--61},
  year      = {1998},
  publisher = {Springer}
}

@article{gneiting2007strictly,
  title     = {Strictly proper scoring rules, prediction, and estimation},
  author    = {Gneiting, Tilmann and Raftery, Adrian E},
  journal   = {Journal of the American statistical Association},
  volume    = {102},
  number    = {477},
  pages     = {359--378},
  year      = {2007},
  publisher = {Taylor \& Francis}
}

@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}

@inproceedings{glorot2010understanding,
  title        = {Understanding the difficulty of training deep feedforward neural networks},
  author       = {Glorot, Xavier and Bengio, Yoshua},
  booktitle    = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages        = {249--256},
  year         = {2010},
  organization = {JMLR Workshop and Conference Proceedings}
}

@article{shafer2008tutorial,
  title   = {A Tutorial on Conformal Prediction.},
  author  = {Shafer, Glenn and Vovk, Vladimir},
  journal = {Journal of Machine Learning Research},
  volume  = {9},
  number  = {3},
  year    = {2008}
}

@book{balasubramanian2014conformal,
  title     = {Conformal prediction for reliable machine learning: theory, adaptations and applications},
  author    = {Balasubramanian, Vineeth and Ho, Shen-Shyang and Vovk, Vladimir},
  year      = {2014},
  publisher = {Newnes}
}

@article{jang2016categorical,
  title   = {Categorical reparameterization with gumbel-softmax},
  author  = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal = {arXiv preprint arXiv:1611.01144},
  year    = {2016}
}

@inproceedings{NEURIPS2018_3ea2db50,
  author    = {Malinin, Andrey and Gales, Mark},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Predictive Uncertainty Estimation via Prior Networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2018/file/3ea2db50e62ceefceaf70a9d9a56a6f4-Paper.pdf},
  volume    = {31},
  year      = {2018}
}

@article{hyndman1996computing,
  title     = {Computing and graphing highest density regions},
  author    = {Hyndman, Rob J},
  journal   = {The American Statistician},
  volume    = {50},
  number    = {2},
  pages     = {120--126},
  year      = {1996},
  publisher = {Taylor \& Francis}
}

@article{chernozhukov2021distributional,
  title     = {Distributional conformal prediction},
  author    = {Chernozhukov, Victor and W{\"u}thrich, Kaspar and Zhu, Yinchu},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {118},
  number    = {48},
  pages     = {e2107794118},
  year      = {2021},
  publisher = {National Acad Sciences}
}

@article{sesia2021conformal,
  title   = {Conformal prediction using conditional histograms},
  author  = {Sesia, Matteo and Romano, Yaniv},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {6304--6315},
  year    = {2021}
}

@article{gupta2022nested,
  title     = {Nested conformal prediction and quantile out-of-bag ensemble methods},
  author    = {Gupta, Chirag and Kuchibhotla, Arun K and Ramdas, Aaditya},
  journal   = {Pattern Recognition},
  volume    = {127},
  pages     = {108496},
  year      = {2022},
  publisher = {Elsevier}
}

@article{izbicki2019flexible,
  title   = {Flexible distribution-free conditional predictive bands using density estimators},
  author  = {Izbicki, Rafael and Shimizu, Gilson T and Stern, Rafael B},
  journal = {arXiv preprint arXiv:1910.05575},
  year    = {2019}
}

@article{wolpert1997no,
  title     = {No free lunch theorems for optimization},
  author    = {Wolpert, David H and Macready, William G},
  journal   = {IEEE transactions on evolutionary computation},
  volume    = {1},
  number    = {1},
  pages     = {67--82},
  year      = {1997},
  publisher = {IEEE}
}

@book{jorion2007value,
  title     = {Value at risk: the new benchmark for managing financial risk},
  author    = {Jorion, Philippe},
  year      = {2007},
  publisher = {McGraw-Hill}
}

@article{loftus2022uncertainty,
  title     = {Uncertainty-aware deep learning in healthcare: a scoping review},
  author    = {Loftus, Tyler J and Shickel, Benjamin and Ruppert, Matthew M and Balch, Jeremy A and Ozrazgat-Baslanti, Tezcan and Tighe, Patrick J and Efron, Philip A and Hogan, William R and Rashidi, Parisa and Upchurch Jr, Gilbert R and others},
  journal   = {PLOS digital health},
  volume    = {1},
  number    = {8},
  pages     = {e0000085},
  year      = {2022},
  publisher = {Public Library of Science San Francisco, CA USA}
}
@article{lambert2024trustworthy,
  title     = {Trustworthy clinical AI solutions: a unified review of uncertainty quantification in deep learning models for medical image analysis},
  author    = {Lambert, Benjamin and Forbes, Florence and Doyle, Senan and Dehaene, Harmonie and Dojat, Michel},
  journal   = {Artificial Intelligence in Medicine},
  pages     = {102830},
  year      = {2024},
  publisher = {Elsevier}
}

@inproceedings{csillag2023amnioml,
  title     = {AmnioML: amniotic fluid segmentation and volume prediction with uncertainty quantification},
  author    = {Csillag, Daniel and Paes, Lucas Monteiro and Ramos, Thiago and Romano, Jo{\~a}o Vitor and Schuller, Rodrigo and Seixas, Roberto B and Oliveira, Roberto I and Orenstein, Paulo},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {37},
  number    = {13},
  pages     = {15494--15502},
  year      = {2023}
}

@article{gawlikowski2023survey,
  title     = {A survey of uncertainty in deep neural networks},
  author    = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
  journal   = {Artificial Intelligence Review},
  volume    = {56},
  number    = {Suppl 1},
  pages     = {1513--1589},
  year      = {2023},
  publisher = {Springer}
}

@article{wu2023application,
  title     = {Application of machine learning in personalized medicine},
  author    = {Wu, Yue and Li, Lujuan and Xin, Bin and Hu, Qingyang and Dong, Xue and Li, Zhong},
  journal   = {Intelligent Pharmacy},
  year      = {2023},
  publisher = {Elsevier}
}

@article{prasad2023tumor,
  title  = {Tumor size estimation and 3D model viewing using Deep Learning},
  author = {Prasad, Venkatavara and Venkataramana, Lokeswari Y and Abhishek, K and Verma, Likhitha and Gokhulnath, T},
  year   = {2023}
}

@inproceedings{sloma2021empirical,
  title        = {Empirical comparison of continuous and discrete-time representations for survival prediction},
  author       = {Sloma, Michael and Syed, Fayeq and Nemati, Mohammedreza and Xu, Kevin S},
  booktitle    = {Survival Prediction-Algorithms, Challenges and Applications},
  pages        = {118--131},
  year         = {2021},
  organization = {PMLR}
}

@article{kvamme2019time,
  title   = {Time-to-event prediction with neural networks and Cox regression},
  author  = {Kvamme, H{\aa}vard and Borgan, {\O}rnulf and Scheel, Ida},
  journal = {Journal of machine learning research},
  volume  = {20},
  number  = {129},
  pages   = {1--30},
  year    = {2019}
}

@article{singh2017stock,
  title     = {Stock prediction using deep learning},
  author    = {Singh, Ritika and Srivastava, Shashi},
  journal   = {Multimedia Tools and Applications},
  volume    = {76},
  pages     = {18569--18584},
  year      = {2017},
  publisher = {Springer}
}

@article{alessandretti2018anticipating,
  title     = {Anticipating cryptocurrency prices using machine learning},
  author    = {Alessandretti, Laura and ElBahrawy, Abeer and Aiello, Luca Maria and Baronchelli, Andrea},
  journal   = {Complexity},
  volume    = {2018},
  pages     = {1--16},
  year      = {2018},
  publisher = {Hindawi Limited}
}

@article{hassanpour2023evaluation,
  title     = {Evaluation of deep neural network in directional prediction of Forex market},
  author    = {Hassanpour, Hamid},
  journal   = {Authorea Preprints},
  year      = {2023},
  publisher = {Authorea}
}

@article{barber1998ensemble,
  title     = {Ensemble learning in Bayesian neural networks},
  author    = {Barber, David and Bishop, Christopher M},
  journal   = {Nato ASI Series F Computer and Systems Sciences},
  volume    = {168},
  pages     = {215--238},
  year      = {1998},
  publisher = {Springer Verlag}
}

@book{neal2012bayesian,
  title     = {Bayesian learning for neural networks},
  author    = {Neal, Radford M},
  volume    = {118},
  year      = {2012},
  publisher = {Springer Science \& Business Media}
}

@article{izbicki2022cd,
  title   = {Cd-split and hpd-split: Efficient conformal regions in high dimensions},
  author  = {Izbicki, Rafael and Shimizu, Gilson and Stern, Rafael B},
  journal = {Journal of Machine Learning Research},
  volume  = {23},
  number  = {87},
  pages   = {1--32},
  year    = {2022}
}

@article{koenker1978regression,
  title     = {Regression quantiles},
  author    = {Koenker, Roger and Bassett Jr, Gilbert},
  journal   = {Econometrica: journal of the Econometric Society},
  pages     = {33--50},
  year      = {1978},
  publisher = {JSTOR}
}

@article{chung2021beyond,
  title   = {Beyond pinball loss: Quantile methods for calibrated uncertainty quantification},
  author  = {Chung, Youngseog and Neiswanger, Willie and Char, Ian and Schneider, Jeff},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  pages   = {10971--10984},
  year    = {2021}
}

@article{angelopoulos2021gentle,
  title   = {A gentle introduction to conformal prediction and distribution-free uncertainty quantification},
  author  = {Angelopoulos, Anastasios N and Bates, Stephen},
  journal = {arXiv preprint arXiv:2107.07511},
  year    = {2021}
}

@article{romano2022conformal,
  title  = {Conformal Prediction Methods in Finance},
  author = {Romano, Jo{\~a}o Vitor},
  year   = {2022}
}

@article{romano2019conformalized,
  title   = {Conformalized quantile regression},
  author  = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}

@article{oliveira2022split,
  title   = {Split conformal prediction for dependent data},
  author  = {Oliveira, Roberto I and Orenstein, Paulo and Ramos, Thiago and Romano, Jo{\~a}o Vitor},
  journal = {arXiv preprint arXiv:2203.15885},
  year    = {2022}
}


@article{sesia2020comparison,
  title     = {A comparison of some conformal quantile regression methods},
  author    = {Sesia, Matteo and Cand{\`e}s, Emmanuel J},
  journal   = {Stat},
  volume    = {9},
  number    = {1},
  pages     = {e261},
  year      = {2020},
  publisher = {Wiley Online Library}
}

@article{gupta2022nested,
  title     = {Nested conformal prediction and quantile out-of-bag ensemble methods},
  author    = {Gupta, Chirag and Kuchibhotla, Arun K and Ramdas, Aaditya},
  journal   = {Pattern Recognition},
  volume    = {127},
  pages     = {108496},
  year      = {2022},
  publisher = {Elsevier}
}

@article{auer2024conformal,
  title   = {Conformal prediction for time series with Modern Hopfield Networks},
  author  = {Auer, Andreas and Gauch, Martin and Klotz, Daniel and Hochreiter, Sepp},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}

@article{abdar2021review,
  title     = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
  author    = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U Rajendra and others},
  journal   = {Information fusion},
  volume    = {76},
  pages     = {243--297},
  year      = {2021},
  publisher = {Elsevier}
}


@article{xia2020uncertainty,
  title     = {Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation},
  author    = {Xia, Yingda and Yang, Dong and Yu, Zhiding and Liu, Fengze and Cai, Jinzheng and Yu, Lequan and Zhu, Zhuotun and Xu, Daguang and Yuille, Alan and Roth, Holger},
  journal   = {Medical image analysis},
  volume    = {65},
  pages     = {101766},
  year      = {2020},
  publisher = {Elsevier}
}

@article{ghesu2021quantifying,
  title     = {Quantifying and leveraging predictive uncertainty for medical image assessment},
  author    = {Ghesu, Florin C and Georgescu, Bogdan and Mansoor, Awais and Yoo, Youngjin and Gibson, Eli and Vishwanath, RS and Balachandran, Abishek and Balter, James M and Cao, Yue and Singh, Ramandeep and others},
  journal   = {Medical Image Analysis},
  volume    = {68},
  pages     = {101855},
  year      = {2021},
  publisher = {Elsevier}
}

@article{mashrur2020machine,
  title     = {Machine learning for financial risk management: a survey},
  author    = {Mashrur, Akib and Luo, Wei and Zaidi, Nayyar A and Robles-Kelly, Antonio},
  journal   = {Ieee Access},
  volume    = {8},
  pages     = {203203--203223},
  year      = {2020},
  publisher = {IEEE}
}

@article{angelopoulos2021gentle,
  title   = {A gentle introduction to conformal prediction and distribution-free uncertainty quantification},
  author  = {Angelopoulos, Anastasios N and Bates, Stephen},
  journal = {arXiv preprint arXiv:2107.07511},
  year    = {2021}
}

@book{klenke2013probability,
  title     = {Probability theory: a comprehensive course},
  author    = {Klenke, Achim},
  year      = {2013},
  publisher = {Springer Science \& Business Media}
}

@article{moon2021learning,
  title     = {Learning multiple quantiles with neural networks},
  author    = {Moon, Sang Jun and Jeon, Jong-June and Lee, Jason Sang Hun and Kim, Yongdai},
  journal   = {Journal of Computational and Graphical Statistics},
  volume    = {30},
  number    = {4},
  pages     = {1238--1248},
  year      = {2021},
  publisher = {Taylor \& Francis}
}



@software{reback2020pandas,
  author    = {The pandas development team},
  title     = {pandas-dev/pandas: Pandas},
  month     = feb,
  year      = 2020,
  publisher = {Zenodo},
  version   = {latest},
  doi       = {10.5281/zenodo.3509134},
  url       = {https://doi.org/10.5281/zenodo.3509134}
}

@article{harris2020array,
  title     = {Array programming with {NumPy}},
  author    = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
               van der Walt and Ralf Gommers and Pauli Virtanen and David
               Cournapeau and Eric Wieser and Julian Taylor and Sebastian
               Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
               and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
               Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
               R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
               G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
               Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
               Travis E. Oliphant},
  year      = {2020},
  month     = sep,
  journal   = {Nature},
  volume    = {585},
  number    = {7825},
  pages     = {357--362},
  doi       = {10.1038/s41586-020-2649-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{paszke2019pytorch,
  title   = {Pytorch: An imperative style, high-performance deep learning library},
  author  = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}

@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}

@article{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  journal={Springer google schola},
  volume={2},
  pages={645--678},
  year={2006}
}
